\sigla*{TIC}{Tecnologias da Informação e Comunicação}

A utilização de reclamações e denúncias como subsídio para melhorar a qualidade dos serviços oferecidos pelo Estado é uma prática comum. Com a crescente evolução de técnicas e capacidade computacional nos últimos anos, é natural observar um aumento do seu emprego na automatização de processos relacionados à classificação e análise desse conjunto imenso de informações não estruturadas que chegam aos órgãos públicos constantemente. Esse entendimento de que associar tecnologia, serviços governamentais e participação social é uma tendência, é afirmado em \citeonline{mehr2017artificial} e enfatizado em \citeonline[p.1]{Chun2010}:

\begin{citacao}
A revolução nas tecnologias da informação e comunicação (TIC) vem mudando não apenas o cotidiano das pessoas, mas também as interações entre governos e cidadãos. O governo digital ou o governo eletrônico começou como uma nova forma de organização pública que suporta e redefine as informações novas e existentes, as comunicações e as interações relacionadas às transações com as partes interessadas (por exemplo, cidadãos e empresas) por meio das TIC, especialmente por meio de serviços de internet, com o objetivo de melhorar o desempenho e os processos do governo.
\end{citacao}

Na Alemanha explorou-se a ideia de que seria possível complementar modelos frequentemente utilizados em investigação forense eleitoral com informações de denúncias de cidadãos a respeito das eleições. Tais modelos utilizam métodos estatísticos, geralmente baseados em contagem de votos e na quantidade de eleitores elegíveis, para avaliar se os resultados de eleições refletem as intenções dos eleitores ou, ainda, avaliar se há indícios de fraudes no processo eleitoral. O uso dessas denúncias é uma forma de incorporar informação contextual e isso pode ser útil para avaliar e refinar os modelos existentes \cite{mebane2016frauds}.

Em \citeonline{liyanage2018matters}, propõe-se uma forma automatizada de priorização de problemas urbanos reportados por cidadãos. O processo sugerido utiliza informações textuais, imagens e votos dados pelos cidadãos em cada problema reportado. Com estas informações o modelo proposto atribui uma probabilidade de o problema ser de alta importância permitindo assim, uma ordenação por prioridade.

Outra forma interessante de automatizar serviços aos cidadãos pode ser vista em \citeonline{Tjandra2015}. Neste trabalho, apresenta-se a forma como a administração da cidade de Surabaia na Indonésia, segunda mais importante do país, resolve o direcionamento de denúncias e outras comunicações. A combinação de algumas técnicas de preprocessamento de textos e o uso de um algoritmo de classificação de documentos permitem apontar para qual departamento da cidade a denúncia ou comunicação deverá ser direcionada.

O que todos os exemplos mencionados anteriormente têm em comum é o uso de informações não estruturadas, reportadas por cidadãos, acerca de assuntos diversos e em uma escala difícil de atender apenas com a análise manual realizada por pessoas. Ao mesmo tempo em que a necessidade de processar e compreender corretamente as demandas torna-se cada vez mais essencial para prestar um serviço melhor ao cidadão, o grande desafio em questão é justamente como fazer isso.

As próximas seções trarão conceitos de NLP e IA que serão importantes para o entendimento das propostas elencadas neste trabalho.

\section{Processamento de Linguagem Natural}

A área de PLN, une diferentes disciplinas como ciência da computação, estatística e linguística computacional, com o objetivo de permitir que máquinas possam, de certa forma, reconhecer informações descritas em linguagem humana. Mais especificamente, ter a capacidade de processar léxica e semanticamente uma linguagem. 


\subsection{Bag of Words}

Há técnicas que permitem que utilize-se apenas o conteúdo léxico de uma linguagem para que se possa extrair informação relevante. Por exemplo, ao transformar-se um texto em um vetor de palavras e suas frequências, é possível comparar e agrupar diferentes textos apenas pelas coocorrências de palavras similares. Esta forma de representação de texto é geralmente descrita na literatura como \sigla{BOW}{\textit{Bag of Words}}. Uma evolução desta representação é proposta por \citeonline{jones1972statistical}. Nesta abordagem, a autora propõe que a especificidade de um termo está diretamente associada a frequência deste termo em um dado documento e inversamente associada ao quão comum este termo é em uma coleção de documentos. Termos que ocorrem em todos os documentos são pouco importantes. Entretanto, termos que ocorrem com maior frequência em um determinado grupo de documentos são mais interessantes para discriminar um grupo de outro. Esta nova abordagem é o \sigla{TF-IDF}{\textit{Term Frequency–Inverse Document Frequency}}, ou a frequência do termo pelo inverso da frequência nos documentos e foi uma das técnicas mais utilizadas por buscadores na internet para a recuperação de informações através do uso de palavras-chave. Outros empregos comuns para esta técnica, ainda nos dias de hoje, devido a sua simplicidade e baixa necessidade computacional são nas áreas de classificação e clusterização de documentos que serão referenciados mais a frente.

Quando usa-se as abordagens descritas acima, é frequente o uso de determinados tratamentos no texto, com o objetivo de eliminar possíveis problemas de padronização além de remover ruídos que podem existir. Em qualquer linguagem, há palavras que não carregam muita informação independentemente do contexto em que estão inseridas. Este conjunto de palavras é comumente chamado de \textit{stopwords}. Algumas \textit{stopwords} da língua portuguesa: de; a; o; que; e; do; da; em; um. Remover as \textit{stopwords} antes de determinadas aplicações de NLP é algo comum. Ainda na mesma linha, pode-se querer achar os radicais das palavras eliminando-se as suas flexões. Para esse propósito é comum o uso de algoritmos de \textit{Stemming} \textit{Lemmatization}. Enquanto \textit{Stemming} apenas corta sufixos e prefixos das palavras mantendo um radical, \textit{Lemmatization} tenta chegar a uma palavra válida no vocabulário e assim é um processo mais lento e que exige desambiguação em alguns casos \cite{Manning1999}.

O que foi discutido até aqui, apenas relaciona textos utilizando pesos atribuídos a palavras no documento sem que sua ordem seja considerada. Dessa forma, apesar de haver uma certa relação que permanece no todo, utilizando estas abordagens, perdemos muita informação relacionada ao contexto em que as palavras foram utilizadas, tanto em relação ao conteúdo quanto em relação ao papel da palavra na estrutura da linguagem. Assim, as próximas técnicas discutidas tentam preservar alguns tipos de informações capturadas de cada palavra dentro de sua sentença. Ou seja, à partir de agora há uma certa captura de aspectos sintáticos e semânticos.

\subsection{Part-of-Speech Tagging}

Ao analisar-se uma palavra dentro de uma sentença, sua posição e relação com as outras palavras lhe conferem um classe gramatical. Conhecer as diferentes funções que uma classe gramatical pode exercer quando está disposta em uma sentença pode ser uma vantagem para resolver ambiguidades da linguagem. A marcação gramatical ou \sigla{POST}{\textit{Part-of-Speech Tagging}} é essencial em algumas tarefas de NLP como o reconhecimento de entidades nomeadas ou \sigla{NER}{\textit{Named Entity Recognition}}. Saber a classe gramatical das palavras em uma sentença pode ser crucial para saber se estamos falando de um nome de pessoa ou de um nome de uma rua no texto, por exemplo \cite{Manning1999}.

\subsection{Word Embedding}

Enquanto as técnicas relacionadas a BOW mapeiam as palavras em vetores esparsos e com valores discretos, as técnicas baseadas em \textit{word embedding} mapeiam sintaxe e semântica em um espaço de representação denominado \sigla{VSM}{Vector Space Model}. Este vetor é, normalmente, um vetor de baixa dimensionalidade e com valores reais. Palavras similares tendem a ter vetores similares e acabam sendo representadas muito próximas no VSM. A principal ideia por trás dessas técnicas é que se você pode prever o contexto em que uma palavra pode aparecer então você pode entender o significado da palavra \cite{goldberg2017neural}.

O algoritmo word2vec proposto por \citeonline{mikolov2013efficient} utiliza esta abordagem e consegue capturar de forma muito eficiente diversas relações semânticas entre as palavras apenas observando suas coocorrências nos textos. O processo de aprendizado é realizado de forma que se deslize sobre o texto uma espécie de janela de tamanho "n" na qual as "n" palavras situadas nela são utilizadas para prever a próxima palavra. 

Já o GloVe, utiliza um mecanismo similar ao word2vec, entretanto utiliza-se também de informações de contexto global de coocorrência de palavras produzindo vetores densos de palavras mais expressivos que em comparação aos gerados pelo word2vec \cite{pennington2014glove}.

O ELMo é um modelo que gera \textit{word embeddings} de uma forma um pouco diferente do word2vec e do GloVe. Nele, significados diferentes de palavras iguais são preservados, assim ele possibilita uma melhor resolução de ambiguidades para uma dada palavra \cite{peters2018deep}.

Por fim, o \sigla{BERT}{\textit{Bidirectional Encoder Representations from Transformers}} criado pelo google em 2018 é considerado o estado da arte para tarefas de PLN. Nele, além de palavras iguais poderem ter representações vetoriais diferentes para contextos diferentes, o algoritmo ainda utiliza todos os tokens próximos a palavra que está sendo avaliada para capturar seu contexto. Ou seja, se outros modelos antes permitiam avaliar os tokens a esquerda ou a direita, à partir do BERT esta captura de contexto passou a ter uma orientação bidirecional. Esse avanço permitiu que o modelo proposto se sobressaísse em 11 diferentes atividades de PLN.

\subsection{Named Entity Recognition}

\sigla{NER}{Named Entity Recognition} é uma das áreas de pesquisa de PLN na qual tenta-se identificar entidades específicas como nomes de pessoas, locais, nomes de empresas e números em textos.