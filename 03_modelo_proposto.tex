\section{Considerações Iniciais}

Neste capítulo pretende-se detalhar melhor o problema a ser resolvido. Posteriormente, será descrito todo o \textit{pipeline} de coleta e tratamentos dos dados para a montagem do \textit{dataset} que será utilizado para treinamento e avaliação do modelo proposto.

Ainda neste capítulo, serão descritos os experimentos realizados, métricas utilizadas e o modelo escolhido. Por fim, serão exibidos alguns resultados obtidos, a partir da utilização do modelo selecionado, em dados reais.

\section{Detalhamento do Problema}

Ao chegar uma nova denúncia através do sistema FalaBR, da OGU, uma equipe de pessoas atua para avaliar as informações presentes na denúncia e decidir se a denúncia está apta a ser apurada ou não. Ou seja, a avaliação da equipe diz respeito a elementos presentes na denúncia que permitem uma apuração dos fatos relatados.

Em geral, estes elementos estão centrados em informações como CPF, CNPJ, Contratos e Convênios entre órgãos públicos e empresas privadas, materialidade (valores monetários descritos na denúncia ou identificados nos contratos e convênios). Com tais informações, a equipe de triagem de denúncias decide se deverá haver apuração (denúncia apta) ou não (denúncia não apta).

Uma grande parte deste trabalho pode ser automatizado. Assim, as consultas realizadas pela equipe, em diversos sistemas, podem ser substituídas por um processo automatizado, desde que seja possível identificar tais entidades no conteúdo das denúncias. Além disso, durante o processo de triagem, a equipe atribui uma nota de 0 a 100 (de 10 em 10) para definir o quão apta é uma denúncia. Por definições internas, notas acima de entre 0 e 50 são consideradas não aptas e notas entre 60 e 100 são consideradas aptas. Tais notas podem ser utilizadas como rótulo para um aprendizado supervisionado.

Assim, sendo possível montar um conjunto de variáveis adequadas é possível propor a criação de um modelo para inferir notas às novas denúncias automaticamente.

O primeiro problema a ser resolvido, então, é a coleta e preparação dos dados. 
A próxima seção detalha os desafios enfrentados e como foram resolvidos.

\section{Coleta de Dados}

O processo de coleta e análise de dados para utilizar em treinamentos de aprendizado de máquina, geralmente, é composto por uma fase de análise exploratória. Em muitos casos, os dados estão estruturados e o maior trabalho é uma análise onde verifica-se duplicidades, dados faltantes, outliers e outras peculiaridades comuns em dados provenientes de sistemas informatizados. 
Já, no caso deste trabalho, não há, diretamente, dados estruturados. As únicas informações que podem ser obtidas são o texto da denúncia e os anexos, quando informados. Assim, a análise das denúncias e o seu uso para um aprendizado supervisionado devem abranger técnicas de NLP.

Como já foi mencionado, há rótulos sendo atribuídos às denúncias. Assim, o \textit{dataset} que será utilizado contém 1489 registros contendo informações da denúncia, anexos e a informação de grau de aptidão. Estes registros compreendem todas as denúncias cadastradas e que possuem rótulo atribuído entre 04/12/2019 e 23/06/2020.

\begin{figure}[htbp]
\caption{Quantidade de denúncias por rótulo}\label{fig_00100_quantidade_denuncias}
\begin{center}
    \includegraphics[width=\columnwidth]{images/fig_00100_quantidade_denuncias.png}
\end{center}
\legend{Fonte: Autor.}
\end{figure}

Conforme pode ser visto na \autoref{fig_00100_quantidade_denuncias}, a maior parte das denúncias recebeu um grau de aptidão 10, ou seja, são denúncias as quais os especialistas da OGU não possuem dúvidas de que as situações reportadas não devem ser apuradas.

\subsection{Extração de Textos dos Anexos}

Ao avaliar os anexos presentes em todas as denúncias cadastradas, identificou-se uma variedade muito grade de tipos de arquivos. Por essa razão, implementar um código, mesmo que utilizando bibliotecas prontas em python não se provou uma alternativa viável. Não foi possível identificar nenhuma biblioteca em python que conseguisse sozinha lidar com qualquer tipo de arquivo. Por outro lado, escrever um código para identificar o tipo de arquivo e realizar a chamada para uma biblioteca especializada no tipo de arquivo em questão não seria uma tarefa trivial e fugiria demais do escopo do trabalho.

\textbf{TODO: melhorar o texto sobre os anexos, explicar sobre conteúdos dos anexos e decisão sobre necessidade de OCR}

Existe, no conjunto de dados avaliado por este trabalho, um total de 1801 anexos. A \autoref{fig_00050_quantidade_anexos_tipo_arquivo}, apresenta a quantidade de anexos de acordo com o tipo de arquivo.

\begin{figure}[htbp]
\caption{Quantidade de anexos por tipo de arquivo}
\label{fig_00050_quantidade_anexos_tipo_arquivo}
\begin{center}
\includegraphics[width=\columnwidth]{images/fig_00050_quantidade_anexos_tipo_arquivo.png}
\end{center}
\legend{Fonte: Autor.}
\end{figure}

\textbf{TODO: parágrafo sobre quantidade de anexos por denúncia - e histograma com faixas 0 a 5 anexos, 6 a 10 ...}

\textbf{TODO: parágrafo sobre descritiva de anexos por denúncia ...}

Assim, para possibilitar a extração dos textos dos anexos, optou-se por utilizar uma ferramenta chamada Apache Tika (disponível em \url{http://tika.apache.org}) implementada em java. Esta ferramenta identifica automaticamente o tipo de arquivo e extrai o conteúdo textual e os metadados. A ferramenta, ainda, é capaz de realizar \sigla{OCR}{\textit{Optical Character Recognition}} quando há imagens dentro de arquivos dos tipo pdf, xls, doc, entre outros ou quando o arquivo é uma imagem.

Desta forma, configurou-se um servidor com a ferramenta instalada na forma de um serviço e utilizou-se uma biblioteca em python para realizar as chamadas ao serviço passando os arquivos que deveriam ter o conteúdo extraído.

\subsection{Extração de Dados Estruturados}

A extração de dados estruturados é a tentativa de identificar certas entidades de interesse no conteúdo do texto da denúncia e seus anexos. As entidades de interesse, neste momento são CPF, CNPJ, convênios, contratos, NIS, nomes de pessoas, palavras fortes e valores.

Apesar de haver diversas formas de reconhecimento de entidades nomeadas, por questões de escopo e tempo, decidiu-se realizar a extração destas informações utilizando expressões regulares. A implementação de regras com expressões regulares para as entidades citadas acima, é simples na maioria dos casos e não exigem um treinamento como no caso dos algoritmos mencionados no \autoref{chapter:revisao_bibliografica}.

\textbf{TODO: detalhar cada tipo de entidade e incluir o código fonte (no anexo? ou no apêndice?) para detecção e validação das entidades}

\textbf{TODO: escrever sobre a quantidade de entidades encontradas nos registros do dataset}

\subsection{Expansão das Informações}

Especialistas da OGU, ao analisar uma denúncia, pesquisam informações a partir do conteúdo das denúncias. Da mesma forma, a etapa de expansão compreende todos os dados estruturados, derivados daqueles encontrados na etapa anterior. 

Por exemplo, para um CPF citado, pode-se avaliar em quais CNPJs este CPF consta como sócio. Em um CNPJ pode-se avaliar os sócios. Em contratos ou convênios pode-se querer avaliar os participantes (órgãos e CNPJs), além do valor envolvido no ato administrativo. Para um nome de pessoa citado, pode-se tentar buscar o CPF caso não haja homônimos na base de CPFs da Receita Federal. Estas informações são extraídas e armazenadas mantendo-se a hierarquia, ou seja, um CNPJ extraído por que um CPF aparece no quadro societário possui um apontamento para o CPF que o originou e a relação ``é sócio de'' é preservada. Além disso, todos os dados estruturados preservam o identificador da denúncia a qual foram extraídos.

\begin{figure}[htbp]
    \caption{Exemplo de extração e expansão de dados estruturados}
    \label{fig_00200_extracao_expansao}
    \begin{center}
        \includegraphics[width=\columnwidth]{images/fig_00200_extracao_expansao.png}
    \end{center}
\legend{Fonte: Autor.}
\end{figure}

A \autoref{fig_00200_extracao_expansao} apresenta um exemplo fictício no qual o processo de extração encontra um CPF. A partir deste CPF dois CNPJs, nos quais o CPF pertence ao quadro societário, são encontrados. Para cada CNPJ encontrado busca-se os CPFs dos demais sócios. Assim, os dados estruturados provenientes da extração são definidos como nível 1, seus dados estruturados derivados nível 2 e assim por diante. Avaliou-se que, acima do nível 3, a quantidade de dados estruturados aumentaria demasiadamente e os mesmos teriam pouca relevância. Assim, na base de dados utilizada para este trabalho serão expandidos e trabalhados apenas informações até o terceiro nível.

\subsection{Qualificação dos Dados Estruturados Encontrados}

O processo de qualificação tem por objetivo extrair outras características para cada dado estruturado encontrado, independentemente do nível. Assim, para um CPF encontrado, por exemplo, pode-se extrair informações como: é ou não servidor público federal, foi demitido da administração pública federal, possui processo administrativo disciplinar, é pessoa exposta politicamente, fez doações a partidos políticos (valor total das doações), entre outras. 

No caso de um CNPJ, como exemplo, podemos verificar se a empresa consta no \sigla{CADIN}{Cadastro Informativo de Créditos não Quitados do Setor Público Federal}, \sigla{CEPIM}{Cadastro de Entidades Privadas sem Fins Lucrativos}, se já teve outras sanções feitas pela administração pública federal, se a empresa fez doações a partidos políticos, entre outras.

As qualificações encontradas podem ser utilizadas como features das denúncias.

\subsection{Preparação do \textit{Dataset}}

Esta etapa consiste na consolidação, em \textit{features} ou características, de todas as informações encontradas nas etapas anteriores. Assim, realiza-se um agrupamento por origem de informações. Algumas características como materialidade são somadas outras como CPF de servidor público são contabilizadas. Dessa forma, para cada denúncia, há um registro com uma quantidade grande de colunas nas quais consolida-se todas as informações encontradas nas etapas anteriores. Por fim, inclui-se o rótulo. A \autoref{tab_00100_visao_geral_dataset} apresenta uma visão em alto nível de como o \textit{dataset} é montado bem como uma explicação da origem de algumas \textit{features} e sua forma de consolidação. 


\begin{table}[htbp]
\caption{Visão geral do dataset após o processamento das denúncias}
\label{tab_00100_visao_geral_dataset}
\tiny
\centering
\begin{tabular}{p{0.175\columnwidth}p{0.55\columnwidth}p{0.2\columnwidth}}
\toprule
     Feature &
     Descrição &
     Forma de Consolidação \\
\midrule

    grau\_aptidao &
    Rótulo definido pelos especialistas &
    Único por denúncia \\
      
    txt\_denuncia &
    Conteúdo da denúncia &
    Único por denúncia \\
    
    txt\_anexos & 
    Conteúdo textual extraído dos anexos 
    &  Único por denúncia \\
    
    nome\_em\_denuncia 
    &  Nomes de pessoas encontrados no texto ou anexos da denúncia. São contabilizados apenas nomes existentes na tabela de pessoa física da receita federal
    & Contagem de ocorrências\\
    
    cpf\_em\_texto\_denuncia
    &  CPFs encontrados no texto ou anexos da denúncia
    & Contagem de ocorrências \\
    
    materialidade
    & Referências a valores monetários encontrados diretamente no texto da denúncia e seus anexos ou em contratos e convênios extraídos das mesmas
    & Soma dos valores \\
    
    denuncia\_anonima
    & Indicador se a denúncia é anônima ou não
    & Binário (1 ou 0) \\
    
    ... 
    &  ... 
    & ... \\
    
    palavras\_fortes
    &  Palavras fortes que possam levar ao entendimento de que há ato suspeito sendo denunciado (fraude, ilegal, desvio, superfaturamento, etc.)
    & Contagem de ocorrências \\
    
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%


Além das 7 \textit{features} e da variável alvo (rótulo) detalhadas na \autoref{tab_00100_visao_geral_dataset}, são extraídas outras 71 \textit{features}. O presente trabalho não detalhará as demais por questões de sigilo, considerando que a metodologia proposta será adotada na instituição em que o autor é vinculado.

\#TODO: detalhar quantidade de dados estruturados por nível encontrados para as denuncias

\#TODO: fazer uma tabela/quadro com informações descritivas min, mediana, media, max de dados estruturados por denúncia

Com o dataset finalizado pode-se iniciar a avaliação dos dados para a proposição de um modelo de classificação.

\section{Experimentos Propostos}

Nesta seção, serão exploradas algumas possibilidades de uso das informações extraídas para construir classificadores. Desta forma, identifica-se algumas alternativas óbvias que devem ser avaliadas em um primeiro momento.

A primeira trata-se de tentar construir um modelo apenas utilizando os dados estruturados extraídos dos textos. A segunda, tentar construir um modelo apenas com as informações textuais contidas nas denúncias e seus anexos. Por fim, uma combinação de ambas as alternativas.

Como mencionado na \autoref{fig_00100_quantidade_denuncias}, a distribuição de denúncias de acordo com os rótulos não é balanceada. Além disso, algumas classes possuem muito poucas observações. Por essa razão, decidiu-se trabalhar com apenas duas classes apta (1), não apta (0). Rótulos com valor entre 10 e 50 foram considerados como 0 e rótulos com valores entre 60 e 100 foram considerados como 1. A figura \autoref{fig_00300_rotulos_duas_classes} apresenta a distribuição das denúncias considerando apenas duas classes. Apesar de o \textit{dataset} permanecer desbalanceado, a quantidade de denúncias por classe agora é representativa.

\begin{figure}[htbp]
    \caption{Distribuição das denúncias por rótulo utilizando 2 classes}
    \label{fig_00300_rotulos_duas_classes}
    \begin{center}
        \includegraphics[width=300pt]{images/fig_00300_rotulos_duas_classes.png}
    \end{center}
\legend{Fonte: Autor.}
\end{figure}

\subsection{Modelo considerando apenas os dados estruturados}
\label{section:modelo_dados_estruturados}

A primeira tarefa realizada nesta etapa foi a avaliação da representatividade das \textit{features} obtidas na tentativa de explicar a variável alvo. O objetivo é diminuir a complexidade do modelo. Por questões de objetividade e tempo, optou-se por um processo automatizado.

O processo escolhido utiliza a classe \textit{SelectFromModel} da biblioteca \textit{scikit-learn} do python \cite{scikit-learn}. Esta classe foi utilizada para selecionar as '\textit{k}' melhores \textit{features} após o treinamento de um \textit{RandomForestClassifier} Estas informações ficam armazenadas no atributo \textit{feature\_importances\_} do modelo. Foram realizados 39 testes para $for\ _k = 2,...,78\ \forall\ k\ \in\ \text{números pares}$, conforme o código abaixo:\newline


\begin{lstlisting}[language=Python]
k_vs_avg_prec_score = []
k_vs_bal_acc_score = []
k_vs_roc_auc_score = []
for k in range(2, X_estruturado_train.shape[1]+1, 2):
    selector_model=RandomForestClassifier(n_estimators=1000, class_weight='balanced')
    selector = SelectFromModel(selector_model, max_features=k, threshold=-np.inf)
    selector.fit(X_estruturado_train, ytrain)
  
    Xtrain2 = selector.transform(X_estruturado_train)
    Xtest2 = selector.transform(X_estruturado_test)
    mdl = RandomForestClassifier(n_estimators=1000, class_weight='balanced')
    mdl.fit(Xtrain2, ytrain)
    p = mdl.predict_proba(Xtest2)[:, 1]
    k_vs_avg_prec_score.append(average_precision_score(ytest, p))
    k_vs_bal_acc_score.append(balanced_accuracy_score(ytest, mdl.predict(Xtest2)))
    k_vs_roc_auc_score.append(roc_auc_score(ytest, p))
\end{lstlisting}

Em cada iteração, é criada uma instância da classe \textit{SelectFromModel} com o parâmetro $max\_features = k$, equivalente a iteração. A classe então é utilizada para selecionar as k melhores \textit{features} e transformar os \textit{datasets} de treino e teste para descartar as não selecionadas. Com o \textit{dataset} de treino transformado, treina-se um novo \textit{RandomForestClassifier} e realiza-se a avaliação do modelo de acordo com as métricas de \textit{Average Precision}, \textit{Balanced Accuracy} e \textit{ROC AUC}. A tabela \autoref{tab_00200_metricas_numero_features} apresenta um resumo dos resultados obtidos.

\begin{table}[htbp]
\caption{Métricas de acordo com o número de features}
\label{tab_00200_metricas_numero_features}
\small
\centering
\begin{tabular}{rrrrr}
\toprule
 Número de Features &  Average Precision &  Balanced Accuracy &  ROC AUC &  Média \\
\midrule
                 10 &             0.4880 &             0.6166 &   0.7150 & 0.6066 \\
                 \textbf{20} &             
                 \textbf{0.5133} &             
                 \textbf{0.6460} &   
                 \textbf{0.7281} & 
                 \textbf{0.6291} \\
                 30 &             0.5108 &             0.6432 &   0.7246 & 0.6262 \\
                 40 &             0.5266 &             0.6531 &   0.7366 & 0.6387 \\
                 50 &             0.5260 &             0.6471 &   0.7345 & 0.6359 \\
                 60 &             0.5263 &             0.6403 &   0.7332 & 0.6333 \\
                 70 &             0.5275 &             0.6487 &   0.7354 & 0.6372 \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%

Com base nos resultados avaliou-se que, a partir de 20 \textit{features}, o ganho médio dos scores não é tão relevante e a utilização de todas as features aumentaria o tempo de treinamento e a complexidade do modelo. Assim, escolheu-se utilizar as 20 melhores features na etapa de modelagem.

A decisão sobre qual algoritmo utilizar para a geração do modelo foi tomada com base na comparação do score de área sob a curva ROC (\textit{roc\_auc}) obtido em diversos modelos testados. Esta métrica foi escolhida por ser uma métrica interessante para avaliação do desempenho do modelo com foco na classe positiva, neste caso a classe 1.

O processo consistiu basicamente em realizar as seguintes etapas para cada algoritmo a ser avaliado:

\begin{itemize}
    \item utilizar a classe \textit{GridSearchCV}, do \textit{scikit-learn} com algumas combinações de hiper-parâmetros e validação cruzada com 10 partições estratificadas para escolher um boa versão de modelo e tornar mais justa a comparação;
    
    \item utilizar a melhor combinação de hiper-parâmetros encontrada para realizar uma validação cruzada com 10 partições e obter as seguintes métricas: \textit{roc\_auc}, \textit{balanced\_accuracy}, \textit{average\_precision} e \textit{f1\_weighted};
    
    \item utilizar a distribuição de scores obtidos para cada métrica e modelo para realizar a escolha do algoritmo mais adequado ao problema.
\end{itemize}

A \autoref{fig_00400_comparacao_score_modelos_dados_estr} apresenta um gráfico de \textit{boxplot} para cada algoritmo avaliado. A ordem em que os algoritmos aparecem, em todos os gráficos, foi definida através da média das quatro métricas avaliadas, em ordem decrescente, da esquerda para a direita. \newline

\begin{figure}[htbp]
    \caption{Distribuição dos scores por métrica e modelo}
    \label{fig_00400_comparacao_score_modelos_dados_estr}
    \begin{center}
        \includegraphics[width=\columnwidth]{images/fig_00400_comparacao_score_modelos_dados_estr.png}
    \end{center}
\legend{Fonte: Autor.}
\end{figure}

O algoritmo escolhido foi o \textit{RandomForestClassifier}, por apresentar o melhor desempenho pela métrica escolhida. Além disso, o algoritmo também performou bem nas outras três métricas avaliadas. 

Com o algoritmo escolhido, o passo seguinte foi a otimização dos hiper-parâmetros. Utilizou-se a biblioteca \textit{skopt} para realizar uma busca automática. Por fim, um novo modelo foi treinado utilizando-se os parâmetros selecionados e todos os dados de treinamento. A \autoref{tab_00300_desempenho_final_modelo_dados_estr} apresenta a matriz de confusão e a \autoref{tab_00400_desempenho_final_modelo_dados_estr} apresenta o relatório de classificação, provenientes da aplicação do modelo nos dados de teste.

\begin{table}[htbp]
\caption{Matriz de confusão do modelo, avaliando-se pelos dados de teste}
\label{tab_00300_desempenho_final_modelo_dados_estr}
\small
\centering
\begin{tabular}{lrr}
\toprule
      {} &  Predito como 0 &  Predito como 1 \\
\midrule
Classe 0 &             176 &              44 \\
Classe 1 &              23 &              55 \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%

\begin{table}[htbp]
\caption{Relatório de classificação, avaliando-se pelos dados de teste}
\label{tab_00400_desempenho_final_modelo_dados_estr}
\small
\centering
\begin{tabular}{lrrrr}
\toprule
          {} & Precisão   &  Revocação & F1-Score &  Suporte \\
\midrule
       Classe 0 &      0.88  &    0.80    &     0.84 &      220 \\
       Classe 1 &      0.56  &    0.71    &     0.62 &       78 \\
             {} &        {}  &      {}    &       {} &       {} \\
       Acurácia &        {}  &      {}    &     0.78 &      298 \\
          Média &      0.72  &    0.75    &     0.73 &      298 \\
Média Ponderada &      0.80  &    0.78    &     0.78 &      298 \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%

O conjunto de testes utilizado possui 298 registros. Assim, como é possível observar na \autoref{tab_00300_desempenho_final_modelo_dados_estr}, tem-se 176 \sigla{VP}{verdadeiros positivos} e 55 \sigla{VN}{verdadeiros negativos} que somam um total de 231 classificações corretas representando uma acurácia geral de aproximadamente 0.78. Ao avaliar os resultados por classe (\autoref{tab_00400_desempenho_final_modelo_dados_estr}), observa-se que o modelo possui uma precisão para a classe 0 bem superior à da classe 1, 0.88 e 0.56, respectivamente. Uma análise mais detalhada do desempenho deste modelo será realizada após os testes com o modelo baseado nos textos que será discutido na próxima seção. Entretanto, é possível afirmar que o modelo gerado possui um desempenho razoável para a tarefa de classificação proposta.

\subsection{Modelo considerando apenas os textos}

Nesta seção será avaliada a possibilidade de criação de um modelo utilizando apenas os textos das denúncias e de seu anexos. Como foi visto no \autoref{chapter:revisao_bibliografica}, há diferentes formas de extrair informações de textos. Elas variam bastante de acordo com o propósito pretendido. 

Primeiramente realizou-se um pré-processamento em todos os documentos com o objetivo de padronizar os textos. Todas as letras foram convertidas para minúsculas, caracteres acentuados foram substituídos por seus equivalentes sem acentuação, \textit{stopwords} foram removidas e espaços excessivos foram eliminados.

Para o propósito deste estudo, será avaliado o TF-IDF como técnica para extração de características de documentos. Como explicado anteriormente, esta técnica combina a contagem de palavras presentes no texto com a ocorrência das mesmas em outros documentos. Com isso, palavras que ocorrem em muitos documentos terão seu peso atenuado enquanto que palavras que ocorrem um conjuntos pequenos de documentos receberão um peso maior.
Como exemplo, a \autoref{tab_00500_exemplo_tfidf} exibe algumas \textit{features} geradas pelo TF-IDF para as primeiras 10 denúncias do \textit{dataset} de treinamento.

\begin{table}[htbp]
\caption{Exemplo de features geradas pelo TF-IDF}
\label{tab_00500_exemplo_tfidf}
\tiny
\centering
\begin{tabular}{lrrrrrrrrrrl}
\toprule
 ... &  ativo &    ato &   atos &  atraves &  atribuicoes &  atuacao &  atual &  auditoria &  augusto &  ausencia &  ... \\
\midrule
 ... & 0.0000 & 0.0988 & 0.0658 &   0.0036 &       0.0115 &   0.0069 & 0.0156 &     0.0000 &   0.0000 &    0.0266 &  ... \\
 ... & 0.0084 & 0.0449 & 0.0184 &   0.0090 &       0.0107 &   0.0045 & 0.0000 &     0.0000 &   0.0027 &    0.0000 &  ... \\
 ... & 0.0000 & 0.0000 & 0.0000 &   0.0000 &       0.0000 &   0.0000 & 0.0000 &     0.0000 &   0.0000 &    0.0000 &  ... \\
 ... & 0.0000 & 0.0000 & 0.0000 &   0.1386 &       0.0000 &   0.0000 & 0.0000 &     0.0000 &   0.0000 &    0.0000 &  ... \\
 ... & 0.0000 & 0.0320 & 0.0145 &   0.0158 &       0.0029 &   0.0087 & 0.0074 &     0.0000 &   0.0007 &    0.0000 &  ... \\
 ... & 0.0000 & 0.0000 & 0.0000 &   0.0000 &       0.0000 &   0.0000 & 0.0000 &     0.0000 &   0.0000 &    0.0000 &  ... \\
 ... & 0.0000 & 0.0000 & 0.0000 &   0.0000 &       0.0000 &   0.0000 & 0.0000 &     0.0000 &   0.0000 &    0.0000 &  ... \\
 ... & 0.0000 & 0.0001 & 0.0002 &   0.0001 &       0.0000 &   0.0003 & 0.0000 &     0.0000 &   0.0006 &    0.0000 &  ... \\
 ... & 0.0000 & 0.0000 & 0.0000 &   0.0000 &       0.0000 &   0.0000 & 0.1226 &     0.0000 &   0.0000 &    0.0000 &  ... \\
 ... & 0.0000 & 0.0000 & 0.0000 &   0.0000 &       0.0000 &   0.0000 & 0.0000 &     0.0000 &   0.0000 &    0.0000 &  ... \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%
Como é possível avaliar na \autoref{tab_00500_exemplo_tfidf}, cada linha representa uma denúncia e cada coluna representa uma palavra. Palavras que não são encontradas na denúncia recebem um peso 0 e palavras que estão presentes na denúncia recebem um peso que é ponderado pela quantidade de vezes que ela está presente no texto e, também, pela sua presença ou não nas demais denúncias analisadas. Esta matriz de palavras por documento será utilizada para o treinamento e comparação dos modelos. Para a comparação dos algoritmos, o TF-IDF foi parametrizado de forma a permitir no máximo 5.000 palavras e só serão aceitas palavras que ocorram, em pelo menos 5 documentos ou no máximo em 50\% dos documentos. Assim, elimina-se palavras que não discriminam muito bem grupos de documentos por serem específicas demais ou genéricas demais.  

A mesma metodologia utilizada na escolha do algoritmo na \autoref{section:modelo_dados_estruturados} foi utilizada para a escolha do algoritmo para este modelo.  A \autoref{fig_00500_comparacao_score_modelos_texto} apresenta um gráfico de \textit{boxplot} com o desempenho de cada modelo. Os modelos foram ordenados na figura pela média das quatro métricas avaliadas, em ordem decrescente, da esquerda para a direita. Aqueles gerados pelos algoritmos  \textit{XGBClassifier} e \textit{KNeighborsClassifier} tiveram, respectivamente, \textit{scores} de 0.718 e 0.725 pela métrica \textit{balanced accuracy}. A pequena diferença nos resultados fez com que as outras métricas fossem avaliadas também. É possível identificar na figura que ambos os modelos têm um desempenho muito próximo para \textit{balanced accuracy} e \textit{f1 weighted}, entretanto, para as métricas de \textit{roc auc} e \textit{average precision} o \textit{XGBClassifier} teve desempenho superior. Por essa razão, escolheu-se o \textit{XGBClassifier} para a otimização de hiper-parâmetros e avaliação final.

\begin{figure}[htbp]
    \caption{Distribuição dos scores por métrica e modelo }
    \label{fig_00500_comparacao_score_modelos_texto}
    \begin{center}
        \includegraphics[width=\columnwidth]{images/fig_00500_comparacao_score_modelos_texto.png}
    \end{center}
\legend{Fonte: Autor.}
\end{figure}

O mesmo processo de otimização utilizado para o modelo da \autoref{section:modelo_dados_estruturados} foi adotado para este modelo. Assim, após a execução da biblioteca \textit{skopt} para encontrar bons hiperparâmetros para o modelo, foi gerado um modelo treinado com todos os dados de treinamento. Os resultados são exibidos nas tabelas \ref{tab_00600_desempenho_final_modelo_texto} e \ref{tab_00700_desempenho_final_modelo_texto}.


\begin{table}[htbp]
\caption{Matriz de confusão do modelo, avaliando-se pelos dados de teste}
\label{tab_00600_desempenho_final_modelo_texto}
\small
\centering
\begin{tabular}{lrr}
\toprule
      {} &  Predito como 0 &  Predito como 1 \\
\midrule
Classe 0 &             170 &              50 \\
Classe 1 &              25 &              53 \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%

\begin{table}[htbp]
\caption{Relatório de classificação, avaliando-se pelos dados de teste}
\label{tab_00700_desempenho_final_modelo_texto}
\small
\centering
\begin{tabular}{lrrrr}
\toprule
             {} & Precisão   &  Revocação & F1-Score &  Suporte \\
\midrule
       Classe 0 &      0.87  &    0.77    &     0.82 &      220 \\
       Classe 1 &      0.51  &    0.68    &     0.59 &       78 \\
             {} &        {}  &      {}    &       {} &       {} \\
       Acurácia &        {}  &      {}    &     0.75 &      298 \\
          Média &      0.69  &    0.73    &     0.70 &      298 \\
Média Ponderada &      0.78  &    0.75    &     0.76 &      298 \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%




% \section{Combinação de Modelos}
pq combinar?

dados presentes na denuncia

dados derivados do que se ve na denuncia ajudam

modelos se complementam?




% \section{Modelo Escolhido}

% \section{Desempenho do Modelo em Produção}

% # TODO: autoencoder

