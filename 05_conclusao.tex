Neste trabalho foi abordado como tema, o processamento automático de denúncias, enviadas através da plataforma Fala.BR. A principal contribuição apresentada, foi uma abordagem baseada na extração de entidades nomeadas. A identificação de entidades nomeadas permitiu que se utilizasse bases de dados oficiais do governo federal para o enriquecimento das informações extraídas diretamente do conteúdo da denúncia e de seus anexos. Tais informações puderam, então, ser utilizadas como variáveis em um modelo de classificação. 

Outra contribuição relevante, apresentada neste trabalho, foi o desenvolvimento de uma metodologia para a extração completa de dados textuais, a partir de diversos formatos de arquivos, utilizando a linguagem de programação Python e o software Apache Tika. O processo utilizado pode ser aproveitado separadamente em qualquer trabalho de processamento de linguagem natural no qual os dados precisem ser adquiridos de arquivos.

A avaliação da metodologia, em dados reais, demonstrou que ela melhora a capacidade de classificação do modelo, tanto isoladamente quanto em combinação com técnicas tradicionais como vetores de palavras por documento, como o TF-IDF. Considera-se que os resultados obtidos pelo modelo final são adequados para o seu emprego no fluxo de trabalho da CGU. Sua adoção permitirá que parte da equipe responsável pela triagem de denúncias possa ser realocada em outras atividades, preservando, ainda assim, a qualidade na triagem das denúncias dos cidadãos.

Entende-se que a metodologia proposta pode ser abstraída e utilizada em outras áreas como, por exemplo, análise de denúncias policiais ou reclamações em ouvidorias de empresas. Obviamente a extração de entidades e a sua qualificação necessitariam de adaptação para o novo propósito. Algumas perguntas essenciais neste caso seriam: quais entidades há interesse em se identificar? Quais informações derivadas destas entidades são interessantes de se obter? Quais bases de dados são necessárias para esta tarefa?

Em trabalhos futuros, pretende-se avaliar técnicas mais avançadas para NER, assim pode-se expandir a quantidade de entidades nomeadas identificáveis, mantendo-se ou até aprimorando-se a qualidade da detecção sem haver a necessidade de um aumento excessivo na capacidade de processamento. Isso permitiria a obtenção de melhores \textit{features} para o modelo baseado em dados estruturados. Assim, o estudo de representações numéricas mais elaboradas para textos, como ELMo, GloVe e BERT serão essenciais, tanto para as novas técnicas de NER quanto para a utilização de arquiteturas de aprendizado profundo, que também serão foco de estudo. Por fim, mas não esgotando todas as possibilidades de melhorias, pretende-se utilizar \textit{autoencoders} com o objetivo de aproveitar o imenso passivo de denúncias não rotuladas para transformar as \textit{features} baseadas em dados estruturados em um conjunto mais relevante de \textit{features}.

