\section{Considerações Iniciais}

Neste capítulo, serão descritos os recursos de hardware e software utilizados, o detalhamento da metodologia utilizada, os experimentos realizados, as métricas utilizadas e o modelo escolhido. Por fim, serão exibidos alguns resultados obtidos a partir da utilização do modelo selecionado aplicado em dados reais.

\section{Recursos Utilizados}
\label{section:recursos_utilizados}

Para a realização dos experimentos, utilizou-se um computador com processador Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz de 8 núcleos lógicos (4 núcleos físicos com  \textit{hyperthreading}), com 32 GB de memória RAM e 500 GB de SSD. O sistema operacional utilizado foi o Windows 10 Home 64 bits. Utilizou-se a distribuição Anaconda (Python 3.7, 64 bits). As principais bibliotecas Python utilizadas são listadas as seguir:

\begin{itemize}
    \item imbalanced-learn==0.7.0;
    \item joblib==0.17.0;
    \item lightgbm==3.0.0;
    \item numpy==1.19.2;
    \item pandas==1.1.3;
    \item scikit-learn==0.23.2;
    \item scikit-optimize==0.8.1;
    \item scipy==1.5.3;
    \item tika==1.24;
    \item xgboost==1.2.1.
\end{itemize}

\section{Experimentos Propostos}
\label{section:experimentos_propostos}

Nesta seção, serão exploradas algumas possibilidades de uso das informações extraídas para construir classificadores. Desta forma, identificam-se algumas alternativas óbvias que devem ser avaliadas em um primeiro momento.

A primeira trata-se de tentar construir um modelo apenas utilizando os dados estruturados extraídos dos textos. A segunda, tentar construir um modelo apenas com as informações textuais contidas nas denúncias e seus anexos. Por fim, uma combinação de ambas as alternativas.

Como mencionado na \autoref{fig_00100_quantidade_denuncias}, a distribuição de denúncias de acordo com os rótulos não é balanceada. Além disso, algumas classes possuem muito poucas observações. Por essa razão, decidiu-se trabalhar com apenas duas classes apta (1), não apta (0). Rótulos com valor entre 10 e 50 foram considerados como 0 e rótulos com valores entre 60 e 100 foram considerados como 1. A figura \autoref{fig_00300_rotulos_duas_classes} apresenta a distribuição das denúncias considerando apenas duas classes. Apesar de o \textit{dataset} permanecer desbalanceado, a quantidade de denúncias por classe agora é representativa.

\begin{figure}[htbp]
    \caption{Distribuição das denúncias por rótulo utilizando 2 classes}
    \label{fig_00300_rotulos_duas_classes}
    \begin{center}
        \includegraphics[width=300pt]{images/fig_00300_rotulos_duas_classes.png}
    \end{center}
\legend{Fonte: Autor.}
\end{figure}

Neste estudo, não há a necessidade de análise e tratamento de dados faltantes. A razão para isso é que todas as informações são obtidas a partir dos textos e anexos das denúncias. Assim, no caso de textos curtos com poucas informações e nenhum anexo a denúncia tende a ser rejeitada, por que poucos ou nenhum dado estruturado é extraído e o próprio texto não traz elementos suficientes para uma apuração. Assim, a maior parte ou todas as \textit{features} de dados estruturados acabam tendo o seu valor zerado. Além disso o TF-IDF gerado para o modelo textual tende a trazer a maior parte de suas \textit{features} com valores baixos ou zerados.

\subsection{Definições Metodológicas}
\label{section:definicoes_metodologicas}

Conforme já informado no \autoref{chapter:metodologia_proposta}, o \textit{dataset} utilizado possui 1489 registros de denúncias rotuladas como apuráveis (aptas) ou não apuráveis (não aptas). Assim, separou-se 80\% (1191) dos registros em um \textit{dataset} de treinamento e 20\% dos registros (298) em um \textit{dataset} de testes. Além disso, a separação em conjuntos de treinamento e testes foi estratificada, isto é, manteve-se a mesma proporção de registros do \textit{dataset} original, para as classes positiva (apurável) e negativa (não apurável), em ambos os conjuntos.

Todos os procedimentos relacionados a seleção de \textit{features}, seleção de modelos e \textit{tunning} de hiper-parâmetros foram realizados utilizando-se o \textit{dataset} de treinamento e as avaliações de desempenho foram realizadas utilizando-se validação cruzada no próprio \textit{dataset} de treinamento. O \textit{dataset} de testes foi utilizado apenas para aferir os scores finais dos modelos.

\subsection{Modelo considerando apenas os dados estruturados}
\label{section:modelo_dados_estruturados}

A primeira tarefa realizada nesta etapa foi a avaliação da representatividade das \textit{features} obtidas na tentativa de explicar a variável alvo. O objetivo é diminuir a complexidade do modelo. Por questões de objetividade e tempo, optou-se por um processo automatizado.

O processo escolhido utiliza a classe \textit{SelectFromModel} da biblioteca \textit{scikit-learn} do Python \cite{scikit-learn}. Esta classe foi utilizada para selecionar as '\textit{k}' melhores \textit{features} após o treinamento de um \textit{RandomForestClassifier} \cite{breiman2001random}. Estas informações ficam armazenadas no atributo \textit{feature\_importances\_} do modelo. Foram realizados 39 testes para $for\ _k = 2,...,78\ \forall\ k=2n,\ n \in \mathbb{N}$.

Em cada iteração, é criada uma instância da classe \textit{SelectFromModel} com o parâmetro $max\_features = k$, equivalente a iteração. A classe então é utilizada para selecionar as k melhores \textit{features} e transformar o \textit{dataset} de treino para descartar aquelas não selecionadas. Com o \textit{dataset} de treino transformado, treina-se um novo \textit{RandomForestClassifier} e realiza-se a avaliação do modelo de acordo com as métricas de \textit{Average Precision}, \textit{Balanced Accuracy} e \textit{ROC AUC}. A tabela \autoref{tab_00200_metricas_numero_features} e a \autoref{fig_00040_kneeplot} apresentam um resumo dos resultados obtidos.

\begin{table}[htbp]
\caption{Métricas de acordo com o número de \textit{features}}
\label{tab_00200_metricas_numero_features}
\small
\centering
\begin{tabular}{rrrr}
\toprule
 Número de \textit{Features} &  Average Precision &  Balanced Accuracy &  ROC AUC  \\
\midrule
                 
                 10 &           0.538429 &           0.623408 &  0.721747 \\
                 \textbf{20} &           
                 \textbf{0.581019} &           
                 \textbf{0.647302} &  
                 \textbf{0.741056} \\
                 30 &           0.579207 &           0.651423 &  0.740430 \\
                 40 &           0.578372 &           0.657877 &  0.739688 \\
                 50 &           0.584122 &           0.654597 &  0.739786 \\
                 60 &           0.584938 &           0.653307 &  0.739417 \\
                 70 &           0.587008 &           0.654857 &  0.740949 \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%

\begin{figure}[htbp]
    \caption{Métricas de acordo com o número de \textit{features}}
    \label{fig_00040_kneeplot}
    \begin{center}
        \includegraphics[width=\columnwidth]{images/fig_00040_kneeplot.png}
    \end{center}
\legend{Fonte: Autor.}
\end{figure}

Com base nos resultados avaliou-se que, a partir de 20 \textit{features}, o ganho dos scores não é tão relevante e a utilização do conjunto completo aumentaria o tempo de treinamento e a complexidade do modelo. Assim, escolheu-se utilizar as 20 mais relevantes na etapa de modelagem.

A decisão sobre qual algoritmo utilizar para a geração do modelo foi tomada com base na comparação do score de área sob a curva ROC (\textit{roc\_auc}) obtido em diversos modelos testados. Esta métrica foi escolhida por ser uma métrica interessante para avaliação do desempenho do modelo com foco na classe positiva, neste caso a classe 1.

O processo consistiu basicamente em realizar as seguintes etapas para cada algoritmo a ser avaliado:

\begin{itemize}
    \item utilizar a classe \textit{GridSearchCV}, do \textit{scikit-learn} com algumas combinações de hiper-parâmetros e validação cruzada com 10 partições estratificadas para escolher um boa versão de modelo e tornar mais justa a comparação;
    
    \item utilizar a melhor combinação de hiper-parâmetros encontrada para realizar uma validação cruzada com 10 partições e obter as seguintes métricas: \textit{roc\_auc}, \textit{balanced\_accuracy}, \textit{average\_precision} e \textit{f1\_weighted};
    
    \item utilizar a distribuição de scores obtidos para cada métrica e modelo para realizar a escolha do algoritmo mais adequado ao problema.
\end{itemize}

A \autoref{fig_00400_comparacao_score_modelos_dados_estr} apresenta um gráfico de \textit{boxplot} para cada algoritmo avaliado. A ordem em que os algoritmos aparecem, em todos os gráficos, foi definida através do score obtido pela métrica \textit{roc\_auc}, em ordem decrescente, da esquerda para a direita. \newline


\begin{figure}[htbp]
    \caption{Distribuição dos scores por métrica e modelo}
    \label{fig_00400_comparacao_score_modelos_dados_estr}
    \begin{center}
        \includegraphics[width=\columnwidth]{images/fig_00400_comparacao_score_modelos_dados_estr.png}
    \end{center}
\legend{Fonte: Autor.}
\end{figure}

O algoritmo escolhido foi o \textit{RandomForestClassifier}, por apresentar o melhor desempenho pela métrica escolhida. Além disso, o algoritmo também teve uma boa performance nas outras três métricas avaliadas. 

O passo seguinte foi a otimização dos hiper-parâmetros. Utilizou-se a biblioteca \textit{skopt} para realizar uma busca automática. Por fim, um novo modelo foi treinado utilizando-se os parâmetros selecionados e todos os dados de treinamento. A \autoref{tab_00300_desempenho_final_modelo_dados_estr} apresenta a matriz de confusão e a \autoref{tab_00400_desempenho_final_modelo_dados_estr} apresenta o relatório de classificação, provenientes das previsões do modelo para os dados de teste.

\begin{table}[htbp]
\caption{Matriz de confusão do modelo, avaliando-se pelos dados de teste}
\label{tab_00300_desempenho_final_modelo_dados_estr}
\small
\centering
\begin{tabular}{lrr}
\toprule
      {} &  Predito como 0 &  Predito como 1 \\
\midrule
Classe 0 &             188 &              32 \\
Classe 1 &              28 &              50 \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%

\begin{table}[htbp]
\caption{Relatório de classificação, avaliando-se pelos dados de teste}
\label{tab_00400_desempenho_final_modelo_dados_estr}
\small
\centering
\begin{tabular}{lrrrr}
\toprule
          {} & Precisão   &  Revocação & F1-Score &  Suporte \\
\midrule
       Classe 0 &      0,87  &    0,85    &     0,86 &      220 \\
       Classe 1 &      0,61  &    0,64    &     0,62 &       78 \\
             {} &        {}  &      {}    &       {} &       {} \\
       Acurácia &        {}  &      {}    &     0,80 &      298 \\
          Média &      0,74  &    0,75    &     0,74 &      298 \\
Média Ponderada &      0,80  &    0,80    &     0,80 &      298 \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%

O conjunto de testes utilizado possui 298 registros. Assim, como é possível observar na \autoref{tab_00300_desempenho_final_modelo_dados_estr}, tem-se 188 verdadeiros negativos e 50 verdadeiros positivos que somam um total de 238 classificações corretas. Ao avaliar os resultados por classe (\autoref{tab_00400_desempenho_final_modelo_dados_estr}), observa-se que o modelo possui uma precisão para a classe 0 (0,87) bem superior à da classe 1 (0,61). O modelo apresenta uma acurácia de 0,80, porém, a acurácia balanceada foi de 0,75. O score \textit{roc\_auc} foi de 0.82 e a curva ROC é apresentada na \autoref{fig_00410_roc_auc_dados_estr}.

\begin{figure}[htbp]
    \caption{Curva Característica de Operação do Receptor (ROC)}
    \label{fig_00410_roc_auc_dados_estr}
    \begin{center}
        \includegraphics[width=\columnwidth]{images/fig_00410_roc_auc_dados_estr.png}
    \end{center}
\legend{Fonte: Autor.}
\end{figure}

Pela análise da curva ROC, entende-se que seria possível elevar o \textit{recall} do modelo diminuindo-se o limiar utilizado para computar as classes positivas e negativas. Com o limiar em 0,5 (padrão), tem-se uma taxa de falsos positivos de 0,14 e um \textit{recall} de 0,64. Entretanto, ao mover-se o limiar para 0.37, tem-se uma taxa de falsos positivos de 0,28 e um \textit{recall} de 0,76. Talvez seja um \textit{tradeoff} interessante pois o falso positivo de uma denúncia é menos problemático do que um falso negativo.

Em geral, avalia-se que a capacidade de distinção entre as classes, aprendida pelo modelo, é bastante razoável.

\subsection{Modelo considerando apenas os textos}
\label{section:modelo_texto}

Nesta seção será avaliada a possibilidade de criação de um modelo utilizando apenas os textos das denúncias e de seu anexos. Como foi visto no \autoref{chapter:revisao_bibliografica}, há diferentes formas de extrair informações de textos. Elas variam bastante de acordo com o propósito pretendido. 

Primeiramente realizou-se um pré-processamento em todos os documentos com o objetivo de padronizar os textos. Todas as letras foram convertidas para minúsculas, caracteres acentuados foram substituídos por seus equivalentes sem acentuação, \textit{stopwords} foram removidas e espaços excessivos foram eliminados.

Para o propósito deste estudo, será avaliado o TF-IDF como técnica para extração de características de documentos. Como explicado anteriormente, esta técnica combina a contagem de palavras presentes no texto com a ocorrência das mesmas em outros documentos. Com isso, palavras que ocorrem em muitos documentos terão seu peso atenuado enquanto que palavras que ocorrem um conjuntos pequenos de documentos receberão um peso maior.
Como exemplo, a \autoref{tab_00500_exemplo_tfidf} exibe algumas \textit{features} geradas pelo TF-IDF para as primeiras 10 denúncias do \textit{dataset} de treinamento.

\begin{table}[htbp]
\caption{Exemplo de \textit{features} geradas pelo TF-IDF}
\label{tab_00500_exemplo_tfidf}
\tiny
\centering
\begin{tabular}{lrrrrrrrrrrl}
\toprule
 ... &  ativo &    ato &   atos &  atraves &  atribuicoes &  atuacao &  atual &  auditoria &  augusto &  ausencia &  ... \\
\midrule
... & 0,0000 & 0,0988 & 0,0658 &   0,0036 &       0,0115 &   0,0069 & 0,0156 &     0,0000 &   0,0000 &    0,0266 &  ... \\
 ... & 0,0084 & 0,0449 & 0,0184 &   0,0090 &       0,0107 &   0,0045 & 0,0000 &     0,0000 &   0,0027 &    0,0000 &  ... \\
 ... & 0,0000 & 0,0000 & 0,0000 &   0,0000 &       0,0000 &   0,0000 & 0,0000 &     0,0000 &   0,0000 &    0,0000 &  ... \\
 ... & 0,0000 & 0,0000 & 0,0000 &   0,1386 &       0,0000 &   0,0000 & 0,0000 &     0,0000 &   0,0000 &    0,0000 &  ... \\
 ... & 0,0000 & 0,0320 & 0,0145 &   0,0158 &       0,0029 &   0,0087 & 0,0074 &     0,0000 &   0,0007 &    0,0000 &  ... \\
 ... & 0,0000 & 0,0000 & 0,0000 &   0,0000 &       0,0000 &   0,0000 & 0,0000 &     0,0000 &   0,0000 &    0,0000 &  ... \\
 ... & 0,0000 & 0,0000 & 0,0000 &   0,0000 &       0,0000 &   0,0000 & 0,0000 &     0,0000 &   0,0000 &    0,0000 &  ... \\
 ... & 0,0000 & 0,0001 & 0,0002 &   0,0001 &       0,0000 &   0,0003 & 0,0000 &     0,0000 &   0,0006 &    0,0000 &  ... \\
 ... & 0,0000 & 0,0000 & 0,0000 &   0,0000 &       0,0000 &   0,0000 & 0,1226 &     0,0000 &   0,0000 &    0,0000 &  ... \\
 ... & 0,0000 & 0,0000 & 0,0000 &   0,0000 &       0,0000 &   0,0000 & 0,0000 &     0,0000 &   0,0000 &    0,0000 &  ... \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%

Na \autoref{tab_00500_exemplo_tfidf}, cada linha representa uma denúncia e cada coluna representa uma palavra. Palavras que não são encontradas na denúncia recebem um peso 0 e palavras que estão presentes na denúncia recebem um peso que é ponderado pela quantidade de vezes que ela está presente no texto e, também, pela sua presença ou não nas demais denúncias analisadas. Esta matriz de palavras por documento será utilizada para o treinamento e comparação dos modelos. Para a comparação dos algoritmos, o TF-IDF foi parametrizado de forma a permitir no máximo 5.000 palavras e só serão aceitas palavras que ocorram, em pelo menos 5 documentos ou no máximo em 50\% dos documentos. Assim, elimina-se palavras que não discriminam muito bem grupos de documentos por serem específicas demais ou genéricas demais.

A mesma metodologia utilizada na escolha do algoritmo na \autoref{section:modelo_dados_estruturados} foi utilizada para a escolha do algoritmo para este modelo.  A \autoref{fig_00500_comparacao_score_modelos_texto} apresenta um gráfico de \textit{boxplot} com o desempenho de cada modelo. Os modelos foram ordenados na figura pela métrica \textit{roc\_auc}, em ordem decrescente, da esquerda para a direita. O algoritmo que teve o melhor desempenho foi o \textit{XGBClassifier} com um \textit{score} de 0,830. Além disso, teve ainda, o melhor desempenho nas outras métricas avaliadas. Por essa razão, escolheu-se o \textit{XGBClassifier} para a otimização de hiper-parâmetros e avaliação final.

\begin{figure}[htbp]
    \caption{Distribuição dos scores por métrica e modelo }
    \label{fig_00500_comparacao_score_modelos_texto}
    \begin{center}
        \includegraphics[width=\columnwidth]{images/fig_00500_comparacao_score_modelos_texto.png}
    \end{center}
\legend{Fonte: Autor.}
\end{figure}

O mesmo processo de otimização utilizado para o modelo da \autoref{section:modelo_dados_estruturados} foi adotado para este modelo. Assim, após a execução da biblioteca \textit{skopt} para encontrar bons hiper-parâmetros para o modelo, foi gerado um modelo treinado com todos os dados de treinamento. Os resultados são exibidos nas tabelas \ref{tab_00600_desempenho_final_modelo_texto} e \ref{tab_00700_desempenho_final_modelo_texto}.


\begin{table}[htbp]
\caption{Matriz de confusão do modelo, avaliando-se pelos dados de teste}
\label{tab_00600_desempenho_final_modelo_texto}
\small
\centering
\begin{tabular}{lrr}
\toprule
      {} &  Predito como 0 &  Predito como 1 \\
\midrule
Classe 0 &             187 &              33 \\
Classe 1 &              31 &              47 \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%

\begin{table}[htbp]
\caption{Relatório de classificação, avaliando-se pelos dados de teste}
\label{tab_00700_desempenho_final_modelo_texto}
\small
\centering
\begin{tabular}{lrrrr}
\toprule
             {} & Precisão   &  Revocação & F1-Score &  Suporte \\
\midrule
       Classe 0 &      0,86  &    0,85    &     0,85 &     220 \\
       Classe 1 &      0,59  &    0,60    &     0,59 &       78 \\
             {} &        {}  &      {}    &       {} &       {} \\
       Acurácia &        {}  &      {}    &     0,79 &      298 \\
          Média &      0,72  &    0,73    &     0,72 &      298 \\
Média Ponderada &      0,79  &    0,79    &     0,79 &      298 \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%


É possível observar na matriz de confusão (\autoref{tab_00600_desempenho_final_modelo_texto}) 187 verdadeiros negativos e 47 verdadeiros positivos que somam um total de 234 classificações corretas. Ao avaliar os resultados por classe (\autoref{tab_00700_desempenho_final_modelo_texto}), observa-se que o modelo possui uma precisão para a classe 0 (0,86) bem superior à da classe 1 (0,59). O modelo apresenta uma acurácia de 0,79, porém, a acurácia balanceada foi de 0,73. O score \textit{roc\_auc} foi de 0,83 e a curva ROC é apresentada na \autoref{fig_00600_roc_auc_texto}.

\begin{figure}[htbp]
    \caption{Curva Característica de Operação do Receptor (ROC)}
    \label{fig_00600_roc_auc_texto}
    \begin{center}
        \includegraphics[width=\columnwidth]{images/fig_00600_roc_auc_texto.png}
    \end{center}
\legend{Fonte: Autor.}
\end{figure}

De forma geral, pode-se afirmar que este modelo teve um desempenho bem parecido com o modelo proposto na \autoref{section:modelo_dados_estruturados}. Talvez levemente inferior. Mas, ainda assim, com um poder de discriminação entre as classes razoável, conforme observa-se na \autoref{fig_00600_roc_auc_texto}.

\section{Combinação dos Modelos}
\label{section:combinacao_modelos}

Nesta seção, avalia-se a possibilidade de combinar os modelos obtidos nas seções \ref{section:modelo_dados_estruturados} e \ref{section:modelo_texto}. O objetivo aqui é identificar se algum tipo de combinação pode melhorar o desempenho da classificação em relação aos desempenhos isolados de cada modelo. 

Após aplicar-se as correlações de Pearson e Spearman nas probabilidades (para a classe positiva) previstas pelos dois modelos, para os dados de teste, obtém-se, respectivamente, os coeficientes 0,6879 e 0,6419. Por esta razão, entende-se que há uma certa quantidade de registros nos quais os modelos possivelmente classificam de forma diferente. Assim, foram testadas 3 combinações possíveis. A menor, a maior e a média das probabilidades, para a classe positiva, previstas pelos dois modelos, para cada registro do conjunto de testes.


\subsection{Combinação dos modelos pela menor probabilidade}

Neste cenário, foram aplicados os dois modelos nos dados de teste. Para cada registro utilizou-se a menor probabilidade obtida entre os modelos e a outra foi descartada. Entende-se que, neste caso, tem-se uma classificação mais pessimista ou conservadora pois se, ao menos, um dos modelos indicar uma probabilidade abaixo de 0,5, o registro será classificado como pertencente à classe 0. O resultado desta combinação pode ser avaliado nas tabelas \ref{tab_00800_desempenho_final_modelo_comb_menor} e \ref{tab_00900_desempenho_final_modelo_comb_menor}.

\begin{table}[htbp]
\caption{Matriz de confusão do modelo, avaliando-se pelos dados de teste}
\label{tab_00800_desempenho_final_modelo_comb_menor}
\small
\centering
\begin{tabular}{lrr}
\toprule
      {} &  Predito como 0 &  Predito como 1 \\
\midrule
Classe 0 &             204 &              16 \\
Classe 1 &              39 &              39 \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%

\begin{table}[htbp]
\caption{Relatório de classificação, avaliando-se pelos dados de teste}
\label{tab_00900_desempenho_final_modelo_comb_menor}
\small
\centering
\begin{tabular}{lrrrr}
\toprule
             {} & Precisão   &  Revocação & F1-Score &  Suporte \\
\midrule
       Classe 0 &      0,84  &    0,93    &     0,88 &      220 \\
       Classe 1 &      0,71  &    0,50    &     0,59 &       78 \\
             {} &        {}  &      {}    &       {} &       {} \\
       Acurácia &        {}  &      {}    &     0,79 &      298 \\
          Média &      0,77  &    0,71    &     0,73 &      298 \\
Média Ponderada &      0,81  &    0,82    &     0,80 &      298 \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%

Esta combinação leva o modelo, de uma forma geral, a rejeitar mais denúncias. Isso ocasiona um aumento de precisão para ambas as classes, entretanto, a revocação ou taxa de verdadeiros positivos cai bastante. Neste caso tem-se um aumento considerável na quantidade de falsos negativos. Esse comportamento não é interessante para o problema em questão. O \textit{ROC AUC score} foi de 0,83 o \textit{balanced accuracy score} foi de 0,71. Houve uma melhora na métrica ROC AUC e uma piora na acurácia balanceada quando comparado com os modelos das seções \ref{section:modelo_dados_estruturados} e \ref{section:modelo_texto}.

\subsection{Combinação dos modelos pela maior probabilidade}

Neste cenário, utilizou-se, para cada registro a maior probabilidade obtida entre os modelos e a outra foi descartada. Entende-se que, neste caso, tem-se uma classificação mais otimista ou arrojada pois se, ao menos, um dos modelos indicar uma probabilidade acima de 0,5, o registro será classificado como pertencente à classe 1. O resultado desta combinação pode ser avaliado nas tabelas \ref{tab_001000_desempenho_final_modelo_comb_maior} e \ref{tab_001100_desempenho_final_modelo_comb_maior}.

\begin{table}[htbp]
\caption{Matriz de confusão do modelo, avaliando-se pelos dados de teste}
\label{tab_001000_desempenho_final_modelo_comb_maior}
\small
\centering
\begin{tabular}{lrr}
\toprule
      {} &  Predito como 0 &  Predito como 1 \\
\midrule
Classe 0 &             171 &              49 \\
Classe 1 &              20 &              58 \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%

\begin{table}[htbp]
\caption{Relatório de classificação, avaliando-se pelos dados de teste}
\label{tab_001100_desempenho_final_modelo_comb_maior}
\small
\centering
\begin{tabular}{lrrrr}
\toprule
             {} & Precisão   &  Revocação & F1-Score &  Suporte \\
\midrule
       Classe 0 &      0,90  &    0,78    &     0,83 &      220 \\
       Classe 1 &      0,54  &    0,74    &     0,63 &       78 \\
             {} &        {}  &      {}    &       {} &       {} \\
       Acurácia &        {}  &      {}    &     0,77 &      298 \\
          Média &      0,72  &    0,76    &     0,73 &      298 \\
Média Ponderada &      0,80  &    0,77    &     0,78 &      298 \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%

Esta combinação leva o modelo, de uma forma geral, a aceitar mais denúncias. Isso ocasiona uma diminuição de precisão para a classe positiva, entretanto, a revocação ou taxa de verdadeiros positivos aumenta consideravelmente. Neste caso, tem-se um aumento considerável na quantidade de falsos positivos. Esse comportamento pode ser aceitável para o problema em questão. O \textit{ROC AUC score} foi de 0,85 o \textit{balanced accuracy score} foi de 0,76. Houve uma melhora em ambas as métricas quando comparado com os modelos das seções \ref{section:modelo_dados_estruturados} e \ref{section:modelo_texto}.


\subsection{Combinação dos modelos pela média das probabilidades}

Neste cenário, utilizou-se, para cada registro a média das probabilidades obtidas entre os modelos. Entende-se que, neste caso, tem-se uma classificação mais equilibrada. O resultado desta combinação pode ser avaliado nas tabelas \ref{tab_001200_desempenho_final_modelo_comb_media} e \ref{tab_001300_desempenho_final_modelo_comb_media}.

\begin{table}[htbp]
\caption{Matriz de confusão do modelo, avaliando-se pelos dados de teste}
\label{tab_001200_desempenho_final_modelo_comb_media}
\small
\centering
\begin{tabular}{lrr}
\toprule
      {} &  Predito como 0 &  Predito como 1 \\
\midrule
Classe 0 &             192 &              28 \\
Classe 1 &              34 &              44 \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%

\begin{table}[htbp]
\caption{Relatório de classificação, avaliando-se pelos dados de teste}
\label{tab_001300_desempenho_final_modelo_comb_media}
\small
\centering
\begin{tabular}{lrrrr}
\toprule
             {} & Precisão   &  Revocação & F1-Score &  Suporte \\
\midrule
       Classe 0 &      0,85  &    0,87    &     0,86 &      220 \\
       Classe 1 &      0,61  &    0,56    &     0,59 &       78 \\
             {} &        {}  &      {}    &       {} &       {} \\
       Acurácia &        {}  &      {}    &     0,79 &      298 \\
          Média &      0,73  &    0,72    &     0,72 &      298 \\
Média Ponderada &      0,79  &    0,79    &     0,79 &      298 \\
\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%

Observa-se um equilíbrio maior entre precisão e revocação para ambas as classes. Entretanto há uma diminuição no número de acertos da classe positiva e, consequentemente, um aumento no número de falsos negativos. O \textit{ROC AUC score} foi de 0,86 o \textit{balanced accuracy score} foi de 0,72. Houve uma melhora para ROC AUC e uma piora para a acurácia balanceada quando comparado com os modelos das seções \ref{section:modelo_dados_estruturados} e \ref{section:modelo_texto}.

\section{Discussão}

A escolha de qual modelo seria mais interessante para o problema tem como baliza um ponto muito importante: enviar uma denúncia fraca ou sem elementos para a apuração é um problema muito menor do que não apurar uma denúncia com elementos suficientes e verdadeira.

A \autoref{tab_001400_desempenho_final_tabela_comparativa} apresenta as principais métricas obtidas para cada experimento apresentado nas seções  \ref{section:experimentos_propostos} e \ref{section:combinacao_modelos}.

\begin{table}[htbp]
\caption{Tabela comparativa dos resultados obtidos para cada modelo}
\label{tab_001400_desempenho_final_tabela_comparativa}
\tiny
\centering
\begin{tabular}{lrrrr}
\toprule
            Modelo &  
            Precisão média  &  
            Acurácia Balanceada & 
            F1-Score (Classe Positiva) &  
            ROC AUC \\
\midrule
              Dados Estruturados           &   0,48  &  0,75 & 0,62 & 0,82 \\
                           Texto           &   0,46  &  0,74 & 0,59 & 0,84 \\
  Combinação pela menor probabilidade      &   0,49  &  0,71 & 0,59 & 0,84 \\
  Combinação pela maior probabilidade      &   0,47  &  0,76 & 0,61 & 0,85 \\ 
  Combinação pela média das probabilidades &   0,46  &  0,72 & 0,59 & 0,86 \\ 

\bottomrule
\end{tabular}
\legend{Fonte: Autor.}
\end{table}%


Entende-se que a metodologia proposta atinge o objetivo de melhorar a classificação de denúncias aptas. O modelo baseado apenas nos dados estruturados evidencia esta afirmação. Ou seja, a derivação de informações a partir de elementos chave encontrados nas denúncias efetivamente colabora com o processo de classificação. Ainda assim, ao avaliar-se as previsões obtidas pelos dois modelos, constata-se que, em muitos casos, ambos se complementam. A consequência disso foi um melhor desempenho obtido na combinação otimista destes modelos em relação aos seus desempenhos isolados.